{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science For The Insurance Industry \n",
    "#### Rahul Birmiwal\n",
    "#### Sept 2018\n",
    "#### Manhattan Life Insurance Company\n",
    "#### Prepared For Mr. Tyler Harris, Data Analytics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Table of Contents: \n",
    "\n",
    "- ** I      Introduction to Data Science & Machine Learning Methods ** \n",
    "- ** II.    The Machine Learning \"Black Box\"** \n",
    "- ** III.   Textual Data Handling - ICD 10 Codes ** \n",
    "- ** IV.    Choice of Response Variable ***y*** -- Adapting To Different Use Cases** \n",
    "- **  V.     Python Data Science for Lapse Risk Modelling ** \n",
    "- **  VI.    Claims Ranking and Logistic Regression -- Kaggle Data ** \n",
    "- ** VII.   Fraudulent Claims Data** \n",
    "- ** VIII.  Using Machine Learning Output For Business Decision Making**\n",
    "- ** IX. Other Provided Files **\n",
    "- ** X Further Reading **\n",
    "- ** XI. An Aside: ARIMA Modelling ** \n",
    "- ** XII. Overview of Use-Case Ideas For Insurance Industry ** \n",
    "- ** XIII. References **\n",
    "\n",
    "\n",
    "The following paper provides a general framework for applying data science and machine learning techniques to life insurance-industry related domains. These domains can include\n",
    "    - Lapse Risk Modelling \n",
    "    - Claims Ranking and Logistic Regression -- Kaggle Data\n",
    "    - Fraudulent Claims Detection\n",
    "    - Using machine learning for business practice. \n",
    "    \n",
    "The paper delves into the above domain-specific chapters in the latter half of the paper. The early portions of this paper provide a quick flash study --  from a foundational knowledge base regarding data science/machine learning to original implementations of machine learning models in Python and other code snippet examples that are pertinent to the latter discussion surrounding solving insurance data problems. \n",
    "\n",
    "This paper comes with various source code files that can hopefully serve as a reference guide for data science practice in Python. \n",
    "\n",
    "Machine learning can be daunting; the field itself is very new beginning really only in the late 20th century. Furthermore there is \"some\" research literature regarding ML for insurance data, but the spectrum is limited. As a result, this paper is borne from a conglomeration of sources, some from graduate school and prolific professors, some from the (minimal) set of research found (see references section), some sources pertain indirectly to the problems at end (such as textual data handling), and some sources are simply how the author would tackle a type of problem. \n",
    "\n",
    "## I. Introduction to Data Science & Machine Learning Methods \n",
    "There are hundreds of brilliant books, papers, websites, etc. on data science and machine learning, and while this section is most certainly a far cry from the depth addressed in these other materials, here we simply address \"fundamental building block\" ideals of DS/ML upon which predictive modelling for the aforementioned two domains (or any other future work) can be based. \n",
    "\n",
    "The term data, which we define as a matrix ***X*** is broad and vague, but can be decomposed into its defining schema, i.e. a relation **R(a, b, c, ..., z)** where *a,b,c...* are the columns are the data. ***X*** is then a matrix representation of ***R*** (without the column names...obviously). Now, pertinent to insurance-industry data, where fields/columns can (often are) textual (or simply non-numeric), we define a decomposition of any data ***X*** into **X.num** and **X.tex**, where X.num is the numeric portion of the data; X.tex likewise the remained/textual/unstructured portion. For simplicity we assume here that X.tex is only textual (string) data. \n",
    "\n",
    "Lastly, we assume there exists a target/response variable within ***X*** (can be numeric, textual, ordinal, categorical, etc). This is the variable the ML algorithms will seek to target. We call this ***_y_*** \n",
    "\n",
    "Under this framework of **X.tex**, **X.num** and ***y*** we bring into discussion machine learning algorithms. Like a craftsmiths' toolbox, the universe of ML algorithms is huge and ever expanding. Fundamentally, a machine learning algorithm, given a set of constraints and d-dimensional feature space (number of columns in **R**), seeks to find the coefficients $\\vec{\\beta} \\in \\mathbf{R}^D$ such that a desired loss (also called \"objective\") function is minimized, where informally the loss function can be described as a function of the difference between the actual *y* values and predicted *y* values.\n",
    "\n",
    "That is: we assume a statistical model that our target ***y*** is a linear combination of the columns in ***X***\n",
    "$$\n",
    "\\begin{align*}\n",
    "    y(\\beta) = \\beta_0x_0 + \\beta_1x_1 + ... + \\beta_dx_d  \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\beta$ a d-dimensional vector are the coefficients of the model. \n",
    "\n",
    "*The model assumes that a \"true\" or \"ideal\" $\\beta$ exists. The whole point of statistical modelling and machine learning is to find, as best we can, that $\\beta$*\n",
    "\n",
    "Machine learning seeks to search, sometimes for days running on an ultra fast computer, a $\\hat{\\beta}$ such that for all possible values in ***y*** that our prediction y-hat: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    y(\\hat{\\beta}) = \\hat{\\beta}_0x_0 + \\hat{\\beta}_1x_1 + ... + \\hat{\\beta}_dx_d  \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "is as close to $y(\\beta)$ as possible, or in otherwords in the most simplest case that, the net error, or loss as is called in ML: $Loss=y(\\beta) - y(\\hat{\\beta})$ is minimized. \n",
    "\n",
    "All loss functions, with matrix $\\mathbf{X} \\in R^{nxd}$ and target response vector/labels $\\mathbf{X} \\in R^{nx1}$ are of the form:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbf{J}(\\beta) &= \\vec{y} - \\vec{y'} + \\text{Regularization}\\\\\n",
    "              &= \\frac{1}{N}\\sum_{i=1}^{d} loss_i(y_i, \\vec{x_i}) + \\mathbf{\\Omega(\\beta)} \\\\\n",
    "    &\\text{where Omega is some penalty/regularization function on beta} \\\\ \n",
    "    &\\text{and loss() is a scalar function between class label y_i and data point vector x_i}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and machine-learning can be written succintly as: \n",
    "$$\n",
    "\\begin{align*}\n",
    "    argmin_{\\vec{\\beta} \\in \\mathbf{R}^D} \\mathbf{J(\\beta)} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "The classical L2-Regularized Regression (OLS Regression) loss-function, for example is: \n",
    "$$\n",
    "\\begin{align*}\n",
    "    F(\\beta) =  \\frac{1}{N}\\sum (y_i - \\vec{x_i}^T\\vec{\\beta})^2 + \\lambda||\\vec{\\beta}||^2 \\\\\n",
    "    &\\text{where we have used F for visual contrast}\n",
    "\\end{align*}\n",
    "$$\n",
    "     \n",
    "Now on a fundamental level **ALL** ML algorithms' loss functions have the form of **J**, which under-the-hood require vector-by-vector operations, (i.e. dot product) and *therefore require numerical values upon which to operate*\n",
    "\n",
    "With that said, \"fancier\" ML algorithms (and not to discount from the research, people, etc. from which they are borne) are simply more complex versions of **J**. For example a neural network has a $ \\mathbf{J} = f(J_1, J_2, J_3,..)$ where the inner Js are merely other other loss functions of the same form as the original **J**. Another example: other algorithms, such as a logistic regression model vs. a linear support vector machine, differ essentially only in their loss function (one uses log-loss, the other some sort of \"hinge loss\"). Takeaway these \"names\" however, and the loss function of any **J** is simply a differential and convex function; fancier ML algorithms, in a simplisitic view, just tweak the mathematical properties of this differential, convex function to the nature of the data/problem at hand and usually have faster solutions to this convex optimization problem. \n",
    "\n",
    "The performance benefit gained from shifting from, say, the basic linear support vector machine with squared hinge loss, to a recursive neural network, may be large. However, *Wang, Yibo & Xu, Wei. Leveraging deep learning with LDA-based text analytics to detect automobile insurance fraud* show that with respect to insurance data, which is a combination of numeric and textual fields (See Section III below), that those fancy models only exhibit slightly improvement. Therefore with regards to this paper, fancier ML models are certainly not worth the complexity required; therefore we need only consider *simple* models. \n",
    "\n",
    "Therefore for *any* data ***X*** such that the full dataset is numeric, any *arbitrary* ML algorithm can work (of course, once determined if the algorithm need be regression or classification). The fact that our data, for example, is pertinent to the insurance industry has no bearing on the underlying mathematics behind the ML algorithms themselves. The domain-specific knowledge, whether that be that the data is for insurance purposes, or modelling climate change, or predicting number of airline travellers on Thanksgiving, can just be prefixed and appended to that machine learning engine. Of course one may say that one can modify the loss function to account for domain-specific knowledge. That is true, and if the situation is simple enough, is feasable. However, when the number of predictors/size of the data is very large, it can be very hard to create a \"custom\" decaying loss function that is guaranteed to be differentiable and convex over $R^D$. For the purposes of this paper in its application of data science to insurance data, we only take the former route.  \n",
    "\n",
    "Problems do arise, however, when textual data, exists. Namely we must define some intermediary mapping from the full alphanumeric space, to only the numeric space. There is no cookie-cutter method to accomplish this, per se. Technically one can apply a hashcode to any string (ala how Python dictionaries or R's named lists work), but this is both generally computational ill-advised (what if the lexicon of ***X.tex*** is gigantic?) as well as theoretically \"dumb\" -- using such one-hot encoding, on the large-scale, would necessitate the original feature vectors be augmented with the \"entire lexicon\" and therefore essentially modify the original ***X*** matrix to be \"infinitely\"-dimensional, but of course where 99% of the covariates in this new matrix are just useless. \n",
    "\n",
    "Consequently, we have a scenario where \"feature engineering\" from **X.tex** will become domain-specific. But once such feature engineeing successfully maps the textual data to numeric data, we can then proceed as aforementioned, using any arbitrary ML algorithm depending on the type of problem. \n",
    "\n",
    "\n",
    "## II. The Machine Learning \"Black Box\"\n",
    "\n",
    "One of the primary use-cases for machine learning is to predict some sort of variable(s). With respect to our domain, that variable could be lapse risk, or whether or not a particular claim is fraudulent, etc. Continuing the above discussion of loss functions, etc. we provide a flash flyover of fundamental machine learning concepts after model selection is determined. \n",
    "\n",
    "We previously described how given a chosen model/loss function, ML can be formally written as: \n",
    "$$\n",
    "\\begin{align*}\n",
    "    argmin_{\\vec{\\beta} \\in \\mathbf{R}^D} \\mathbf{J(\\beta)} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "What does this mean? Let us take the case of classical OLS (L2-Regularized) Regression. We have the loss function, in matrix notation: \n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbf{J(\\beta)} = (\\mathbf{y} - \\mathbf{X}^T\\vec{\\beta})^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now we seek to find the coefficients $\\beta$ that minimize J. Let's go back to Calculus I and recall how to minimize a function: take the derivative and set it to zero. \n",
    "\n",
    "A similar process is done through an algorithm called *gradient descent* which is the underbelly for almost all ML training methods. An analogy is thus: suppose we are at the top of a very tall hill. Our objective is to reach the bottom of the hill, but being a computer, we are (blind). Thus we propose the following solution to reach the hill's bottom: take a step, then survey the ground. After each step, determine the direction that descends steepest. Then take another step in that direction. Repeat the process until we reach the bottom of the hill! \n",
    "\n",
    "Mathematically the hill is our loss function we are trying to minimize; the gradient is the direction of steepest descent. Now we are guaranteed to reach the bottom of the hill if one, the loss function is convex (analogy here being that our hill doesn't have \"mountains\" poking out of it on the way down), and differentiable (analogy here being that there are no holes one can fall into and get forever \"stuck\"). Gradient descent then writes as: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{initialize:} \\hspace{1cm} \\mathbf{\\beta}_{0} = \\mathbf{\\vec{0}}\\in R^d \\\\\n",
    "    \\text{WHILE (not converged) do:} \\\\ \n",
    "    \\text{update:} \\hspace{1cm} \\mathbf{\\beta_t} &= \\mathbf{\\beta}_{t-1} - \\nabla{J(\\beta_{t-1})}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Fancier, more elaborate ML algorithms employ sped-up and \"smarter\" versions of gradient descent, hence the names i.e. Accelerated Gradient Descent, Boosting Support Vector Machine, Adaboost, but the concept is still the same as trumbling to the bottom of an undulating hill. \n",
    "\n",
    "Thus to use gradient descent, we need the gradient $\\nabla{J(\\beta)}$. Using matrix calculus, we quickly obtain that for OLS it is \n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\vec{\\nabla{J(\\beta)}} = -2\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}^T\\vec{\\beta})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## III. Textual Data Handling - ICD 10 Codes \n",
    "\n",
    "While the clear majority of data pertinent to insurance claims, etc. will be numeric, the one textual bottleneck deals with descriptions of the claimaint(s) medical conditions/history/etc themselves. Fortunately, the medical world has developed a statistical classification system to designate the plethora of medical-related \"keywords.\" Akin to the Dewey Decimal System, the ICD-10 system is a classification system that groups, subgroups and subdivides medical keywords.  \n",
    "\n",
    "There are different \"chapters\" within the ICD-10 system, chapters (I,II, ...XXII) where i.e. chapter XIII is defined in the range M00-M99 (Diseases of the musculoskeletal system and connective tissue)\n",
    "\n",
    "Codes within a specific chapter are defined in i.e. M00.xx -- M99.xx where xx is in 1,2,3....99. This classification system is therefore akin to a tree-like structure, where: \n",
    "1. first three characters of the ICD-10 string determine chapter\n",
    "2. The region M00-M99 is subdivided into subregions. For example M05-M14 is for \"Inflammatory polyarthropathies\"\n",
    "3. The .X defines each of the individual names of diseases/medical field/etc. within its subregion. \n",
    "\n",
    "With this in mind, while the full ICD-10 string i.e. \"M05.3\" may be useful, what is perhaps *more* useful is the name of the subregion (i.e. \"Inflammatory polyarthropathies\"). \n",
    "\n",
    "So what is the data scientist presented in this situation? We have non-numeric data, i.e. \"M05.3\" or \"H45.82\". There are several approaches one could take here. We provide a brief discussion on those before a fuller discussion on perhaps the most malleable and \"smartest\" approach: Latent Dirichelet Allocation (LDA). \n",
    "\n",
    "### 1a. Strict \"Hash\" Function\n",
    "\n",
    "For reasons discussed hitherto, not a good idea. \n",
    "\n",
    "### 1b. Tokenization/Indexation \n",
    "\n",
    "Instead of hashing, a similar method would be to map each unique non-numeric element in a given data series to a token/index. For example given a set S=[\"hello my name is\", \"fox\", \"I like Olive Garden\", \"3XHU, \"fox\"], its tokenization form would be S = [0,1,2,3,1]. The code snippet below is an example of applying this method to each column in a Python pandas data frame: \n",
    "```python\n",
    "for (train_name, train_series) in pd_x_train.iteritems(): \n",
    "    if train_series.dtype == 'O':\n",
    "        #for objects: factorize\n",
    "        pd_x_train[train_name], tmp_indexer = pd.factorize(pd_x_train[train_name])\n",
    "```\n",
    "### 2. Alpha-Numeric Split \n",
    "\n",
    "Of note is that the data structure here is a concatenation of alphabetical and numerical elements. One could split this datum into two separate data fields along the alpha-numeric line (i.e. M, and 05.3). \n",
    "While a good idea perhaps, this is not recommended as the ICD-10 classification system, in contrast, splits *after* the M.xx element, not on \"M\" itself. This intuition propels us to the third approach: \n",
    "\n",
    "### 3. Classification Split \n",
    "\n",
    "Split on the ICD-10 classification boundaries themselves. I.e. M05.3 --> M05, 3.0. This *can* work, however the number there are 26 letters x 100 numbers = 2600 possible entries for the \"first\" datum here, which may be \"too specific.\" That is, what may result is a similar scenario to as if we one-hot encoded into a massive feature-space. \n",
    "\n",
    "### 4. Classification Split II. \n",
    "\n",
    "Along the same lines as number 3, we can slightly modify the approach to yield more useful data. Each chapter is subdivided into different subchapters, each subchapter housing a set of homogenous medical fields. For example Chapter II has the following classification structure: \n",
    "`\n",
    "1 C00–D48 – Neoplasms\n",
    "\n",
    "    1.1 (C00–C14) Malignant neoplasms, lip, oral cavity and pharynx\n",
    "    1.2 (C15–C26) Malignant neoplasms, digestive organs\n",
    "    1.3 (C30–C39) Malignant neoplasms, respiratory system and intrathoracic organs\n",
    "    1.4 (C40–C41) Malignant neoplasms, bone and articular cartilage\n",
    "    1.5 (C43–C44) Malignant neoplasms, skin\n",
    "    1.6 (C45–C49) Malignant neoplasms, connective and soft tissue\n",
    "    1.7 (C50–C58) Malignant neoplasms, breast and female genital organs\n",
    "    1.8 (C60–C63) Malignant neoplasms of male genital organs\n",
    "    1.9 (C64–C68) Malignant neoplasms, urinary organs\n",
    "    1.10 (C69–C72) Malignant neoplasms, eye, brain and central nervous system\n",
    "    1.11 (C73–C75) Malignant neoplasms, endocrine glands and related structures\n",
    "    1.12 (C76–C80) Malignant neoplasms, secondary and ill-defined\n",
    "    1.13 (C81–C96) Malignant neoplasms, stated or presumed to be primary, of lymphoid, haematopoietic and related tissue\n",
    "    1.14 (C97) Malignant neoplasms of independent (primary) multiple sites\n",
    "    1.15 (D00–D09) In situ neoplasms\n",
    "    1.16 (D10–D36) Benign neoplasms\n",
    "    1.17 (D37–D48) Neoplasms of uncertain or unknown behaviour\n",
    "`\n",
    "\n",
    "Therefore instead of splitting i.e. C34.11 into C34 and 11, we would split into \"Malignant neoplasms, respiratory system...\" and 11. \n",
    "\n",
    "Notice that the key jump here is that we are going deeper into the \"fullname\" of fuller text description of the original datum. \n",
    "\n",
    "This approach can work, but it posits two issues: First, is that the second datum, (i.e. 11) has no stand-alone value: the numerical value 11 *only* has meaning within the context of its subchapter. As such it *cannot* be interpreted as a normal numerical value. Second: we now have a much bigger text string (i.e. \"Malignant neoplasms, respiratory system and intrathoracic organs\"). What does one do now?\n",
    "\n",
    "### 5. Latent Dirichelet Allocation \n",
    "\n",
    "We now proceed to the most abstract, but adaptable, text data mining approach: Latent Dirichelet Allocation (LDA). In short, LDA is an inferencing or generative model that seeks, for a collection of **documents**, and a hyperparameter **number of topics**, to find a collection of **topics** such that each document can be seen a distribution of words drawn from its maximal-likelihood topic. Developed by Ng, Blei, Jordan in their canonical paper \"Latent Dirichelet Allocation,\" LDA is widely used in academia and industry and itself and its variants are a gold-standard approach to textual data mining. \n",
    "\n",
    "We provide the introduction to the paper here for further elaboration: \n",
    "\n",
    "\n",
    "*We describelatent Dirichlet allocation(LDA), a generative probabilistic model for collections of\n",
    "discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each\n",
    "item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in\n",
    "turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of\n",
    "text modeling, the topic probabilities provide an explicit representation of a document...\n",
    "In this paper we consider the problem of modeling text corpora and other collections of discrete\n",
    "data.The goal is to find short descriptions of the members of a collection that enable efficient\n",
    "processing of large collections while preserving the essential statistical relationships that are useful\n",
    "for basic tasks such as classification, novelty detection, summarization, and similarity and relevance\n",
    "judgments* \n",
    "\n",
    "\n",
    "How do we use this? \n",
    "\n",
    "First in the data files provided with this report there is a file `icd_code_description_map.csv` which contains the full mapping between ALL codes, and their full descriptions, i.e. : \n",
    "\n",
    "`\n",
    "...\n",
    "...\n",
    "A759,Typhus fever,,,,\n",
    "A770,Spotted fever due to Rickettsia ricket,tsii,,,\n",
    "A771,Spotted fever due to Rickettsia conori,i,,,\n",
    "A772,Spotted fever due to Rickettsia siberi,ca,,,\n",
    "A773,Spotted fever due to Rickettsia austra,lis,,,\n",
    "A7740,Ehrlichiosis,,,,\n",
    "A7741,Ehrlichiosis chafeensis [E. chafeensis,],,,\n",
    "A7749,Other ehrlichiosis,,,,\n",
    "A778,Other spotted fevers,,,,\n",
    "A779,Spotted fever,,,,\n",
    "A78,Q fever,,,,\n",
    "A790,Trench fever,,,,\n",
    "A791,Rickettsialpox due to Rickettsia akari,,,,\n",
    "A7981,Rickettsiosis due to Ehrlichia sennets,u,,,\n",
    "A7989,Other specified rickettsioses,,,,\n",
    "A799,Rickettsiosis,,,,\n",
    "A800,Acute paralytic poliomyelitis,,,,\n",
    "A801,Acute paralytic poliomyelitis,,,,\n",
    "A802,Acute paralytic poliomyelitis,,,,\n",
    "A8030,Acute paralytic poliomyelitis,,,,\n",
    "A8039,Other acute paralytic poliomyelitis,,,,\n",
    "A804,Acute nonparalytic poliomyelitis,,,,\n",
    "A809,Acute poliomyelitis,,,,\n",
    "...\n",
    "...\n",
    "`\n",
    "\n",
    "where i.e. A759 is codeform for A75.09. \n",
    "\n",
    "Let our ***original*** claims data be of the form ***X*** = **R(......., ICD_10, ......)** where ICD_10 is a string format of the above codes i.e. \"A7891\"\n",
    "The approach is thus: \n",
    "1. Select desired number of topics, $\\kappa$ (hyperparameter)\n",
    "2. Let all description strings above form the training document / training data matrix $\\mathbf{\\Gamma}$. That is, for each string datum $s_i$ in the right-hand column of icd_code_description_map, create an array $\\mathbf{\\Gamma} = [s_1,s_2...,s_N]$.\n",
    "3. Train a LDA Classifier, `clf`, on $\\mathbf{\\Gamma}$: clf.train($\\mathbf{\\Gamma}$, $\\kappa$)\n",
    "\n",
    "\n",
    "4. For all ICD_10 datum $w$ in ***X***, let $\\hat{w}$ <- `clf.classify(w)` where the output (topic number) of _classify()_ is the argmax  infered probability over all possible topics\n",
    "5. For all ICD_10 datum  $w$ in ***X*** replace: $w$ <- $\\hat{w}$ respectively; $w$ is now of datatype numeric, representing the topic number for medicical condition originally defined via ICD_10 and `icd_code_description_map.csv` \n",
    "6. Do until satisfied: Perform cross-validation/training on this updated ***X*** using a (different) classifier and algorithm of choice (i.e. Linear SVM using accelerated gradient descent with hyperparamter $\\lambda$). If necessary, repeat steps 1-6 using a different $\\lambda$ and/or $\\kappa$\n",
    "\n",
    "To show that the above is actually not that complex, we display its Python implementation source code (note all source code files will be in Python)\n",
    "\n",
    "```python\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 0:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "def preprocess_helper(train_text_df):\n",
    "        data_text = train_text_df[[0]]\n",
    "        data_text['index'] = data_text.index\n",
    "        processed_doc = data_text[0].map(preprocess)\n",
    "        dictionary =  gensim.corpora.Dictionary(processed_doc)\n",
    "        return(processed_doc, dictionary)\n",
    "\n",
    "class LDA_Classifier():\n",
    "    def __init__(self, kappa, gamma):\n",
    "        self.kappa = num_topics\n",
    "        self.gamma = gamma\n",
    "        self.dictionary = None\n",
    "        self.processed_doc = None\n",
    "        self.lda_model = None\n",
    "\n",
    "    def preprocess(self):\n",
    "        (self.processed_doc, self.dictionary) = preprocess_helper(self.gamma)\n",
    "\n",
    "    def train(self):\n",
    "        bow_corpus = [self.dictionary.doc2bow(doc) for doc in self.processed_doc]\n",
    "        self.lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=self.kappa, id2word=self.dictionary, passes=2, workers=2)\n",
    "\n",
    "    def classify(self, unseen_text_string):\n",
    "\n",
    "        bow_vector = self.dictionary.doc2bow(preprocess(unseen_text_string))\n",
    "        for index, score in sorted(self.lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "            print(\"Score: {}\\t Topic: #{} {}\".format(score, index, self.lda_model.print_topic(index, 5)))\n",
    "\n",
    "        print(\".....NOW RETURNING THE ARGMAX TOPIC BASED ON SCORE...\")\n",
    "        return(sorted(self.lda_model[bow_vector], key=lambda x: x[1])[-1][0])\n",
    "\n",
    "def example_run():\n",
    "    root = Path(__file__).parents[1]\n",
    "    print(root)\n",
    "    icd_mapping = pd.read_csv(root / \"relevant_data_files\" / \"icd_code_description_map.csv\",delimiter=\",\")\n",
    "\n",
    "    data_text = icd_mapping[['Des']]\n",
    "    data_text['index'] = data_text.index\n",
    "    data_text.columns=[0,'index']\n",
    "\n",
    "    my_lda_classifier = LDA_Classifier(kappa=25,gamma=data_text )\n",
    "    my_lda_classifier.preprocess()\n",
    "    my_lda_classifier.train()\n",
    "\n",
    "    test_string = \"typhoid fever and swelling\"\n",
    "    (classified_topic) =  my_lda_classifier.classify(test_string)\n",
    "    print(classified_topic)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nltk.download('wordnet')\n",
    "    np.random.seed(2018)\n",
    "    example_run()\n",
    "\n",
    "```\n",
    "\n",
    "## IV. Choice of Response Variable ***y*** -- Adapting To Different Use Cases\n",
    "\n",
    "Hitherto we have kept our discussion on the nature of the training matrix ***X***. Now we turn to the response vector ***y***, namely discussing different forms ***y*** may take, or ought to take, depending on end-user use cases. For each case, we provide ML algorithm and the corresponding Python scikit-learn model syntax. Full examples may be found in `examples.py` or `demo.py`. From scratch implementation of the weighted-OVR multiclass model (see below) can be found in `linearsvm.py`\n",
    "\n",
    "### IV.i Binary Classification \n",
    "\n",
    "$y \\in {0,1}$: one may use a linear support vector machine, which tends to be faster than logistic regression due to absence of large exponentiation/log. \n",
    "```python\n",
    "class sklearn.svm.LinearSVC(penalty=’l2’, loss=’squared_hinge’, dual=True, tol=0.0001, C=1.0, multi_class=’ovr’, fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000\n",
    "```\n",
    "\n",
    "### IV.ii Multiclass Classification\n",
    "\n",
    "$y \\in K=[0,1,2...N]$ (values in K are arbitrary). \n",
    "A multiclass problem can be solved using approaches revolving around an \"ensemble\" of individual binary classifiers. Here we discuss the weighted-OVR, or weighted one-v-rest approach. The approach is, for the number of classes N, train N distinct binary classifiers (say Linear SVM), where the $jth$ classifier, $0 \\leq j \\leq N$ *transforms* the original $\\mathbf{y}$ (which recall has N unique) values a new  $\\hat{\\mathbf{y}} $ such that \n",
    "\n",
    "$$\\hat{\\mathbf{y_i}} = 1, \\hspace{0.5cm} \\text{if y_i == j} \\\\\n",
    "   \\text{and zero otherwise} \n",
    "$$\n",
    "\n",
    "Classification/prediction is then done by doing a \"majority vote\": classify a new data point $y_{new}$ against all N binary classifiers and select the class that does $y_{new} = argmax(classifier.score())  \\hspace{0.2cm} \\forall \\hspace{0.2cm}\\text{classifiers in ensemble}$\n",
    "\n",
    "A natural question to ask is how does such ensemble account for skewed data -- i.e. when there are many more occurences of particular class number(s) in the data set compared to others. We can \"rebalance\" the ensemble by applying an approriate weight $\\rho_i\\hspace{0.2cm} \\forall \\hspace{0.2cm} i \\in K$ to the $\\hat{y}=1$ terms (where recall in the ensemble, we have \"aliased\" each of the N classes using a binary classifier and its jth position within the ensemble). \n",
    "\n",
    "It has been shown (Z. Akata, F. Perronnin, Z. Harchaoui and C. Schmid, \"Good Practice in Large-Scale Learning for Image Classification,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 3, pp. 507-520, March 2014.\n",
    "doi: 10.1109/TPAMI.2013.146) that the weighted-OVR superiorly outperforms a vanilla OVR multiclass model. \n",
    "\n",
    "Because it can be slightly confusing, for the sake of completeness, and to assist in further extensions if the reader desires, we show the mathematical derivaion  of the loss function for a weighted *binary* classifier that is used in the weighted-OVR ensemble. Full implementation and demonstration found in `linearsvm.py` \n",
    "\n",
    "A weighted-OVR classifier applies a weighting parameter rho to the positive and negative terms of the loss function to each of its individual classifiers within the ensemble: \n",
    "\n",
    "$$ \n",
    " \\begin{align*}\n",
    "  J &= \\frac{\\rho}{N_+}\\sum_{i:y_i=1}Loss(y_i,x_i,\\beta)    &+ \\frac{1-\\rho}{N_-}\\sum_{k:y_k=-1}Loss(y_k,x_k,\\beta) + \\Omega(\\beta)\n",
    " \\end{align*}\n",
    "$$\n",
    "* Where N+ is the number of positive ys in the data; likewise for N- \n",
    "* For example, setting rho=0.5 yields the standard 1/N term in front of the summation. \n",
    "\n",
    "The huberized/smooth hinge loss function for the linear support vector machine $h()$ writes as in http://qwone.com/~jason/writing/smoothHinge.pdf (2), if we take $ z_i = y_iX_i^T\\beta - 0.5 $\n",
    "\n",
    "It therefore becomes immediate that a weighted cost function for OVR is: \n",
    "\n",
    "$$ \n",
    " \\begin{align*}\n",
    "  J = \\frac{\\rho}{N_+}\\mathbf{\\vec{1}}^T(h(\\mathbf{y_+}\\circ\\mathbf{X_+}^T\\beta)) \\\\ \n",
    "  + \\frac{1-\\rho}{N_-}\\mathbf{\\vec{1}}^T(h(\\mathbf{y_-}\\circ\\mathbf{X_-}^T\\beta)) \\\\ \n",
    "  +  \\Omega(\\beta)\n",
    " \\end{align*}\n",
    "$$\n",
    "\n",
    "Where $\\circ$ denotes the element-wise or Hadamard product\n",
    "\n",
    "And the gradient is: \n",
    "$$ \n",
    " \\begin{align*}\n",
    "  \\nabla{J} &= \\frac{\\rho}{N_+}\\mathbf{X_+}^T\\mathbf{y_+}\\circ(h'\\mathbf(\\mathbf{y_+}\\circ\\mathbf{X_+}^T\\beta))\\\\  +& \n",
    "  \\frac{1-\\rho}{N_-}\\mathbf{X_-}^T\\mathbf{y_-}\\circ(h'\\mathbf(\\mathbf{y_-}\\circ\\mathbf{X_-}^T\\beta)) + 2\\lambda\\beta\n",
    " \\end{align*}\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{X_{+}}$, $\\mathbf{y_{+}}$, etc are the sub-matrices of ***X***,***y*** corresponding to the positive (negative) indices of the vector y\n",
    "\n",
    "### IV.iii Regression (OLS, L2, L1 Regularized)\n",
    "\n",
    "$\\mathbf{y}$ is a continuous random variable. We can apply standard OLS Regression, L2-Regularization or L1 (Lasso) Regularization. The Regularization function term, written as $\\Omega{(\\beta)}$ above,  has the effect of ensuring that the $\\beta$ coefficients are as small as possible, which can be vitally important in some use cases. \n",
    "\n",
    "OLS Regression\n",
    "```python\n",
    " class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)[source]\n",
    " ```\n",
    "\n",
    "L2 Regression \n",
    "```python \n",
    " class sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=’auto’, random_state=None)[source]\n",
    "```\n",
    "L1/Lasso Regression\n",
    "```python\n",
    " class sklearn.linear_model.Lasso(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection=’cyclic’)[source]\n",
    "```\n",
    "\n",
    "### IV.iv Takeaway\n",
    "\n",
    "The takeaway here is that *independent* of the training data ***X*** matrix, we can select appropriately from the aforementioned models to fit the end use case / objective / $\\mathbf{y}$. The user may also engineer a \"new\" output $\\mathbf{y}$ from that of the original, as well, to fit end use cases/objectives. For example, the user may discretize the original $\\mathbf{y}, y_i\\in R$. a continuous real-valued random variable, to a new $\\mathbf{y}, y_i\\in K=[0,1,...,N]$ (transforming from regression to multiclass classification), by some type of binning method.  \n",
    "\n",
    "## V. Python Data Science for Lapse Risk Modelling \n",
    "\n",
    "### V.i Data Characteristics\n",
    "\n",
    "The author of this paper was provided a sample data set from Mr. Tyler Harris of Manhattan Life Insurance Company. We refer to that dataset as \"*this* dataset\", in contrast to other datasets found in research literature, such as that below. While *this* dataset is a fine start, the reason for exploring other datasets is that *this* dataset has a small number of covariates, and does not fully capture all information that might be available and/or prescient to the task of predictive modelling. \n",
    "\n",
    "Boodhun and Jayabalan, \"Risk prediction in life insurance industry using supervised learning algorithms\" delineate the following variables upon which a lapse risk model can be defined: \n",
    "\n",
    "**Categorical Variables**\n",
    "    - Gender (*)\n",
    "    - Smoker (*)\n",
    "    - Married (*)\n",
    "    - Product Info\n",
    "        - Variables defining the product applied for \n",
    "        \n",
    "** Numerical Variables**\n",
    "    - Person Information (*)\n",
    "        - Age, BMI, Weight, Gender, etc. \n",
    "    - Employment Info \n",
    "    - Insurance History\n",
    "    - Geographic Information (*)\n",
    "        - City, state, region\n",
    "    - Medical History\n",
    "    - Family History \n",
    "    - Medical Keywords/ICD-10 Codes \n",
    "        - Variables relating to medical keywords associated with the application \n",
    "\n",
    "** Response Variable**\n",
    "\n",
    "    -- Lapse Category/Risk/Level (can be binary or ordinal) \n",
    "    \n",
    "The variables marked with asterisk are present in _this_ dataset. It seems to be of high importance to be able to join auxillary datasets pertinent to the other fields; in particular, data pertaining to medical history (i.e. do they have a history of high-risk diseases), and past insurance history (i.e. when was the last time they filed a claim) would seem most apropos. \n",
    "\n",
    "Another interesting observation is exactly how the response variable should be constructed. Boodhun and Jayabalan use an 8-level ordinal variable for lapse risk, this dataset uses a binary variable {0,1}. Obviously choice of response variable depends on the underwriter/user of these predictive models -- perhaps they want a continuous numerical scale signifying risk level, or perhaps a straight-to-the-point classification between {low, med, high} (a 3-level ordinal variable). \n",
    "\n",
    "In this section, we explore using both the binary form (as currently present in this dataset), as well as an artifically constructed k-level ordinal variable for multiclass classification. \n",
    "\n",
    "### V.ii Binary Response Scenario\n",
    "\n",
    "#### Python Dependencies \n",
    "\n",
    "- Python 3.X\n",
    "- Anaconda Distribution (i.e., simply install Anaconda Navigator for Windows)\n",
    "- Libraries: \n",
    "    * sklearn\n",
    "    * numpy \n",
    "    * pandas\n",
    "    * pathlib \n",
    "    \n",
    "- Data: \n",
    "    * *lapse_modeling_sample_data.csv*\n",
    "    \n",
    "#### Load Data \n",
    "\n",
    "```python\n",
    "data_folder = Path(\"\")\n",
    "pd_x_train = pd.read_csv(data_folder / \"lapse_modeling_sample_data.csv\", keep_default_na=False)\n",
    "```\n",
    "\n",
    "#### Data Preprocessing \n",
    "\n",
    "1. Remove any rows with 'NA' type \n",
    "2. Convert text fields to ordinal (numerical) data type using `pandas.factorize()` -- essentially a hashmap between textual data and a numeric equivalent \n",
    "3. Split into **X** training matrix, and **Y** response vector \n",
    "4. Standardize data\n",
    "\n",
    "```python\n",
    "pd_x_train.reset_index\n",
    "pd_x_train = pd_x_train.dropna(how='any')\n",
    "\n",
    "for (train_name, train_series) in pd_x_train.iteritems(): \n",
    "    if train_series.dtype == 'O':\n",
    "        #for objects: factorize\n",
    "        pd_x_train[train_name], tmp_indexer = pd.factorize(pd_x_train[train_name])\n",
    "    \n",
    "pd_y_train = pd_x_train['Lapsed']\n",
    "pd_x_train = pd_x_train.drop(['Lapsed'],axis=1)\n",
    "pd_x_train = pd_x_train.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
    "X_train = pd_x_train.values\n",
    "scaler = StandardScaler() \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "y_train = pd_y_train.values \n",
    "```\n",
    "\n",
    "#### Cross-Validation / Training / Classifying Test \n",
    "\n",
    "`my_cross_validation.py` contains the functions used to perform training on this dataset. In summary the average accuracy on the holdout set with $\\lambda=1000$ was ~94%\n",
    "\n",
    "### V.iii Multiclass Classification Scenario \n",
    "\n",
    "Suppose instead that the end-user wanted an ordinal scale as output i.e. low,med,high risk. We artificially create data to model this scenario: \n",
    "\n",
    "Here instead of binary classification $y \\in [0,1]$, the end use cases require lapse risk be a discrete random variable of many classes $y \\in [0,1,2,...K]$ where the $kth$ class is a measure of lapse risk on this discrete scale. This can be interpreted as lapse risk being *[\"very low, low, medium, high, very high\"...etc]*\n",
    "\n",
    "Although the current data has $y \\in [0,1]$, we can create an artifical data set off this, thereby transforming into a multiclassification problem. We define a new target variable $\\mathbf{z}$ where\n",
    "\n",
    "$\n",
    "  \\forall  i = 1...length(y), let : \\\\\n",
    "   \\begin{equation}\n",
    "      z_i = \\left \\{\n",
    "      \\begin{aligned}\n",
    "        &unif(0,K), && \\text{if}\\ y_i=1 \\\\\n",
    "        &0, && \\text{otherwise}\n",
    "      \\end{aligned} \\right.\n",
    "    \\end{equation} \\\\\n",
    "    \\\\\n",
    "    \\text{where K is user-defined}\n",
    "$\n",
    "\n",
    "We demonstrate with K=3 in the code snippet here: \n",
    "```python\n",
    "for (train_name, train_series) in pd_x_train.iteritems(): \n",
    "    if train_series.dtype == 'O':\n",
    "        #for objects: factorize\n",
    "        pd_x_train[train_name], tmp_indexer = pd.factorize(pd_x_train[train_name])\n",
    "    \n",
    "pd_y_train = pd_x_train['Lapsed']\n",
    "pd_x_train = pd_x_train.drop(['Lapsed'],axis=1)\n",
    "pd_x_train = pd_x_train.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
    "\n",
    "##create z\n",
    "\n",
    "pd_z_train = np.asarray([np.random.randint(low=0,high=K)  if i==1 else 0 for i in pd_y_train])\n",
    "```\n",
    "\n",
    "Proceeding like before, training and testing on the cross-validation function, yields similarly average accuracy on the holdout set with $\\lambda=1000$ at ~94%. The above demonstrates that like binary classification, multiclassification is a valid approach if the end use-cases necessitate. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"coefficient_plot.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.iv Coefficient Analysis \n",
    "\n",
    "\n",
    "\n",
    "The \"person-level\" coefficients of relevance are: \n",
    "    * PARTY_LASTNAME\n",
    "    * PARTY_MIDDLENAME\n",
    "    * PARTY_ADDRESS1\n",
    "    * PARTY_ADDRESS2\n",
    "    * PARTY_BIRTHDATE\n",
    "    * PARTY_EMAIL\n",
    "    * PARTY_FIRSTNAME\n",
    "    * PARTY_FULLNAME\n",
    "   \n",
    "We see in the above plot coefficients graph that some of these do have significant coefficient weightage w.r.t absolute value. Therefore there is some overfit and we must remove these fields in a new training data set. \n",
    "\n",
    "### V.v Remarks / Other Performance Metrics \n",
    "\n",
    "Lapse risk modelling seems to work with high efficiacy, provided that the performance metric is classification accuracy. Classification accuracy (error) is most commonly used, hence for its usage here. However, depending on end user use-cases, other performance metrics may be desired. Surveying research literature, not much was found regarding such topic; all papers surveyed utilized the standard classification accuracy metric. \n",
    "\n",
    "But suppose a different metric was indeed desired? How might one proceed? There are several options: \n",
    "1. If the metric is simply a one-to-one conversion from the old metric -- i.e. transforming from a continuous scale to a {low, med, high} scale, then this scenario has been discussed above. \n",
    "2. If we are in the linear regression regime, then beyond the canonical R^2 coefficient of determination, other metrics exist, such as \n",
    "    -- AIC \n",
    "    -- BIC\n",
    "    -- MSE \n",
    "    -- etc.\n",
    "    \n",
    "Developing metrics such as the above would require *post-processing*; that is we have the following pipeline for regression: \n",
    "\n",
    "`DATA -> DATA PREPROCESS -> ML Module -> Output R^2 --> * <-- Feedback loop/cross-validation until R^2 satisfied`\n",
    " \n",
    "Using metrics such as minimizing AIC on a linear regression model would entail manually calculating AIC after the 'Output R^2' step, and instead of performing cross-validation/etc using R^2, the reference point then enacts upon this extended calculation in AIC. The same loss functions, model would be the same. \n",
    "\n",
    "`DATA -> DATA PREPROCESS -> ML Module -> Output R^2 --> AIC --> * <-- Feedback loop/cross-validation until AIC satisfied`\n",
    "\n",
    "In this example AIC is simply an extended metric. The end-user could also use any type of custom metric, and analogous to the above pipeline, perform cross-validation against this custom metric instead of R^2.  \n",
    "\n",
    "Of note is that in the model training, gradient descent would *still* be a convex optimization problem minimizing MSE, as in classical fashion. In this regard, these custom metric models are not techncially seeking optimal solutions against said custom benchmark; rather they merely offer a \"hack\" method of incorporating other metrics into the overall problem-solving environment; the true efficicacy of said \"hack\" is therefore case-by-case dependent. \n",
    "\n",
    "Now, there may exist other loss function equivalents that say, minimize AIC instead of MSE. However, mathematics behind these other loss functions are beyond the scope of this paper, and need independent research. \n",
    "\n",
    "## VI Claims Ranking and Logistic Regression -- Kaggle Data \n",
    "\n",
    "Although sample data was not provided, example data can be found on the Kaggle website: https://www.kaggle.com/c/bnp-paribas-cardif-claims-management. The description, from Kaggle, reads: \n",
    "\n",
    "```You are provided with an anonymized dataset containing both categorical and numeric variables available when the claims were received by BNP Paribas Cardif. All string type variables are categorical. There are no ordinal variables.\n",
    "\n",
    "The \"target\" column in the train set is the variable to predict. It is equal to 1 for claims suitable for an accelerated approval.\n",
    "\n",
    "The task is to predict a probability (\"PredictedProb\") for each claim in the test set.\n",
    "```\n",
    "\n",
    "Thus the task at hand here is to output a probability \"score\" for each claim. Notice that this use-case differs from the prior; as now instead of a multiclassification problem we deal with a continuous random variable. Furthermore it asks us to create a probability for each entry, but where is that probability value coming from? \n",
    "\n",
    "The solution is to tackle this type of problem using logistic regression, which deals solely with probabiltities in its classification pursuit. \n",
    "\n",
    "Recall that the standard linear regression statistical model assumes a hypothesis: \n",
    "\n",
    "$$ \n",
    " \\begin{align*}\n",
    "  y = \\beta_0x_0 + \\beta_1x_1 + ... + \\beta_{d}x_{d} \\\\\n",
    "    = \\mathbf{x}^T\\beta\n",
    " \\end{align*}\n",
    "$$\n",
    "\n",
    "We may rewrite this as\n",
    "\n",
    "$$ \n",
    " \\begin{align*}\n",
    "  y  = h(\\beta_0x_0 + \\beta_1x_1 + ... + \\beta_{d}x_{d}) \\\\\n",
    "    = h(\\mathbf{x}^T\\beta)\n",
    " \\end{align*}\n",
    "$$\n",
    "\n",
    "where $h(x)$ is called the \"link\" function in statistical nomenclature. It simply means that the response variable is a function of a linear combination of the inputs. For the case of linear regression, $h(x)$ is the $identity()$ function\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "For logistic regression, probabiltiies are incorporated into this mathematical boiling pot using a different link function, the *logit* function. \n",
    "\n",
    "$$ \n",
    " \\begin{align*}\n",
    "  y = h(x) = g(\\mathbf{x}^T\\beta) = \\frac{1}{1+e^{-\\mathbf{x}^T\\beta}}\n",
    " \\end{align*}\n",
    "$$\n",
    "\n",
    "where $g(z)$ is the $sigmoid$ function. \n",
    "\n",
    "here, $y$ is now defined as a probability (this is by definition of logistic regression). We have that the probability of y==1 given data ***X*** and $\\beta$ is $g(\\mathbf{x}^T\\beta)$\n",
    "\n",
    "That's it. Obviously it is beyond the scope of this paper to delve into the mathematical derivations of logistic regression, but it can be concluded that if the end-use case is a *probability*, then a logistic classifier is the go-to model. \n",
    "\n",
    "Note that logistic regression is used for binary classification (multi classification) by taking the above one step further. That is, the canonical form is to let classification for a new target $y_i$ be: \n",
    "$$\n",
    "\\begin{align*}\n",
    "&y_i = 1 \\hspace{0.2cm}  g(\\mathbf{x}^T\\beta) \\geq 0.5 \\\\\n",
    "&y_i = 0 \\hspace{0.1cm} \\text{ otherwise}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Regardless, logistic regression deals solely in the probability space. \n",
    "\n",
    "A logistic regression code snippet example is below: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load.iris() #load iris dataset \n",
    "X = iris.data[:,0:2] #only use first two features for viz. \n",
    "y = iris.target \n",
    "\n",
    "lamb = 0.1 #lambda \n",
    "clf = LogisticRegression(C=lamb, penalty='l2') #create Logistic Classifier \n",
    "clf.fit(X,y) #train it \n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "#Get probabilities\n",
    "probas = clf.predict_proba(X) #get probabilities that y==1 for each data point in X\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII Fraudulent Claims Data\n",
    "\n",
    "There is no sample data for an exercise in detecting fraudulent claims. Most of the publically available literature out there deal with domains such as credit card fraud, etc. Nonetheless, the blueprint is the same: \n",
    "\n",
    "1. If the data is wholly numeric, or such that non-numeric variables are categorical or ordinal and easily tokenized/indexed, then apply the ML algorithm of choice. \n",
    "2. If the data contains textual fields, then a nuanced approach must be taken akin to the analysis done regarding Latent Dirichelet Allocation. \n",
    "\n",
    "\n",
    "#### Potential Use Cases/Response Variable \n",
    "\n",
    "Although we do not have data, we can imagine what the end use cases entail. First, the user might want to know the probability that a given claim is fraudulent. This problem can be approached analogous to that discussed in Section VI. Second, the user may want an ordinal scale of {not likely, somewhat likely, very likely}. The problem is then a multiclassification problem analogous to that in Section V. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. Using Machine Learning Output For Business Decision Making\n",
    "\n",
    "At the end of the day w.r.t. applied data science for an insurance firm, the data science methods employed are only as good/useful as their impacts, or lack thereof, for the business. As such we provide a simple scenario below to illustrate how the \"result\" of machine learning utilization can be piped into a cost-balancing decision. \n",
    "\n",
    "The example scenario is: suppose we are a business with a budget of B dollars allocated towards detecting fraudulent claims. There are unit costs of incorporating various strategies involving ML (and likewise not employing ML). We want to determine whether it is useful to spend this budget on using machine learning to first -- predict claims with high likelihood of being fraudulent, and second -- spend money on the \"processing costs\" i.e. call and argue for \"one's money back or reimbursement\" (we do not know the correct terminology here...). The alternative would be to do nothing and keep the money B. \n",
    "\n",
    "The constraints are: \n",
    "\n",
    "- Cost of calling to argue a claim ($x_i$) is fradulent (per claim) = C\n",
    "- The \"gain\" of having identified a fraudulent claim is G\n",
    "- Probability $P[x_i=fraud \\hspace{0.3cm}\\vert \\hspace{0.3cm}\\mathbf{X}, \\beta] = p_i$\n",
    "- Management has a budget of B allocated to fraud detection \n",
    "- There are N claims \n",
    "- The \"cost\" of holding onto a fraudulent claim is R\n",
    "- There are $\\rho$ percent of fraud claims, on average, in a given dataset of size N\n",
    "\n",
    "#### Remarks\n",
    "1. If do not do anything: \n",
    "$$\n",
    "\\mathbf{E}[*] = B -\\rho NR\n",
    "$$\n",
    "2. The cost of processing to determine a claim is fraudulent is a sunk cost \n",
    "$$\n",
    "\\mathbf{E}[\\text{process} \\hspace{0.3cm}\\vert \\hspace{0.3cm} x_i] = C\n",
    "$$\n",
    "\n",
    "3. The expected value of doing something for a claim is\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{E}[* \\hspace{0.3cm}\\vert \\hspace{0.3cm} x_i] \\\\\n",
    "= \\mathbf{E}[* \\hspace{0.3cm}\\vert \\hspace{0.3cm} x_i, process,fraud] + \\mathbf{E}[* \\hspace{0.3cm}\\vert \\hspace{0.3cm} x_i, process, nofraud] \\\\\n",
    "= Gp_i - C\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "4. The total expected value of doing something for the maximum allowable number of claims is: \n",
    "\n",
    "Noting that the allowable number of claims to process if $\\frac{B}{C}$ then \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{E}[*\\hspace{0.3cm}\\vert \\hspace{0.3cm} \\Gamma, B ] = \\left(\\sum_{\\forall \\hspace{0.1cm} x_i \\in \\Gamma} (Gp_i - C)\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where $\\Gamma$ is a subset of the full dataset of claims; there are $\\frac{B}{C}$ number of claims in $\\Gamma$ (naturally), and also naturally, the user might want to say $\\Gamma$ =  top $\\frac{B}{C}$ number of claims in ***X*** with the highest probability of fraud, again where the probability is determined by logistic regression. \n",
    "\n",
    "5. The net expected value, given gamma and budget B is then \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{E}[Net\\hspace{0.3cm}\\vert \\hspace{0.3cm} \\Gamma, B ] = \\left(\\sum_{\\forall \\hspace{0.1cm} x_i \\in \\Gamma} (Gp_i - C)\\right)-\\rho NR + B\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that all the parameters except for $\\Gamma$ and $p_i$ are 'fixed' -- they are the constraints. Also $\\Gamma$ itself is a function of the $p_i$, as choosing a subset of the most-likely fraudulent items depends on the probabilities as well. Therefore the entire expected value expression can be seen as a function $g()$ of the logistic regression performance, aka classification error $\\epsilon$\n",
    "\n",
    "6. Decision Boundary \n",
    "\n",
    "A basic decision boundary would therefore be \n",
    "$$\n",
    "\\begin{align*}\n",
    "g(\\epsilon) = \\mathbf{E}[Net\\hspace{0.3cm}\\vert \\hspace{0.3cm} \\Gamma, B ] > 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "The last point implies that the performance strength of a logistic regression model, as expected, can dictate business decision-making. If the overall classification error/mean-squared error is high, then the inequality may not hold and the decision would be to abort. Parallely, the selection of $\\Gamma$ given $\\epsilon$ does have significant impact; what if there are better methods to \"picking the claims you think are most fraudulent\" than a simple sort on probability of fraud. Maybe one draws $\\Gamma \\sim Normal_{\\mathbf{X}}(\\mu,\\sigma)$ where $\\mu$ is some cut-off probability. This regime our problem-solving environement is open-ended. \n",
    "\n",
    "Note that more abstractly, these type of problems are known as constrained optimization problems. There are several generic methods for solving these: method of Lagrange multiples, linear and quadratic programming, etc. Please see Section X: Further Reading for more information. \n",
    "\n",
    "## IX. Other Provided Files \n",
    "\n",
    "* `demo.py`: examples of scikit learn, as well as Rahul Birmiwal's multiclass classifier (and sklearn LinearSVC)\n",
    "* `linearsvm.py`: From-scratch implementations of Linear & Kernel Multiclass SVM by Rahul Birmiwal \n",
    "* `my_cross_validation.py`: Cross-validation functions used for testing\n",
    "* `plot_coefficients.py`: Function that takes a trained classifier and generates bar chart showing coefficient weights\n",
    "* `textual_data_handling.py`: Associated Python source code for LDA, etc. \n",
    "* `icd_code_description_map.csv`: CSV file of the ICD-10 Code Mapping \n",
    "* `BNPParibas_trainData.csv`: Sample insurance data from BNP Paribas & Kaggle. Variable names here are anonymous\n",
    "\n",
    "## X. Further Reading \n",
    "\n",
    "* Informative post on Latent Dirichelet Allocation in Python: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "* Mathematically-heavy, but outstanding class notes from Stanford CS on Machine Learning from the prolific professor and a leader in the field of data science Andrew Ng: http://cs229.stanford.edu/notes/cs229-notes1.pdf\n",
    "* Constrained Optimization: https://en.wikipedia.org/wiki/Constrained_optimization\n",
    "* Geospatial Data Handling: Folium / Leaflet\n",
    "* K-Nearest Neighbors in Python: https://www.python-course.eu/k_nearest_neighbor_classifier.php\n",
    "\n",
    "\n",
    "## XI. An Aside: ARIMA Modelling:\n",
    "\n",
    "Hitherto we have discussed essentially \"static\" data. However, some types of insurance data (i.e. sales) are more appropriately handled as *time-series data*. If this is the case, one can utilize various time-series forecasting methods (instead of regression). The most famous and widely-used family of methods is the ARIMA suite. \n",
    "\n",
    "The ARIMA family is a standard algorithm suite used for time-series modelling and forecasting. While beyond the complexities of this paper, the ARIMA model assumes that any time-series signal is borne from the following elements : \n",
    "1.\tSeasonality: refers to fluctuations in the data related to calendar cycles\n",
    "2.\tTrends: the overall pattern of the series \n",
    "3.\tCyclical: Consists of patterns that are not seasonal. \n",
    "\n",
    "These three parameters are essentially hyperparameters to the underlying model. An excellent approach to ARIMA using the R language may be found here: https://otexts.org/fpp2/arima-r.html\n",
    "\n",
    "An equivalent code snippet showing implementation in Python is shown below. We assume there exists some dataframe data: \n",
    "```python \n",
    "from pyramid.arima import auto_arima\n",
    "\n",
    "stepwise_model = auto_arima(data, start_p=1, start_q=1,\n",
    "                           max_p=3, max_q=3, m=12,\n",
    "                           start_P=0, seasonal=True,\n",
    "                           d=1, D=1, trace=True,\n",
    "                           error_action='ignore',  \n",
    "                           suppress_warnings=True, \n",
    "                           stepwise=True)\n",
    "\n",
    "print(stepwise_model.aic())\n",
    "```\n",
    "\n",
    "\n",
    "## XII. Overview of Use-Case Ideas For Insurance Industry \n",
    "\n",
    "Here we provide a general list of use-case ideas pertinent to the insurance industry. For each we provide the type of ML problem one could use to assist in tackling that given case: \n",
    "\n",
    "1. Risk Assessment / Underwriter \n",
    "    - all ML models (multiclassification, regression, etc). \n",
    "    - however, output of ML model ought to be transformed to an ordinal scale like {low, med, high risk}\n",
    "2. Fraud Detection \n",
    "    - most simply binary classification\n",
    "3. Marketing. For example, effects on effects on marketing on product sales \n",
    "    - regression (i.e. modelling percent change/decrease in product sales) \n",
    "    - geographic / demographic targeting \n",
    "        - would entail feature engineering of geographic data and geospatial libraries like Folium/Leaflet\n",
    "        - could use clustering (i.e. *K-Nearest Neighbors Classifier*) to cluster geospatially then pipe into multiclass classifier \n",
    "4. Customer Experience. For example, integrating customer feedback (text) \n",
    "    - text mining/LDA\n",
    "5. Lapse Risk (see section on Lapse Risk Above)\n",
    "6. \n",
    "\n",
    "## XIII. References \n",
    "\n",
    "1. https://medium.com/@josemarcialportilla/using-python-and-auto-arima-to-forecast-seasonal-time-series-90877adff03c\n",
    "2. Flavia Barsotti, Xavier Milhaud, Yahia Salhi. Lapse risk in life insurance: correlation and contagion\n",
    "effects among policyholders’ behaviors. 2016. <hal-01282601v2>\n",
    "3. Boodhun, N. & Jayabalan, M. Complex Intell. Syst. (2018) 4: 145. https://doi.org/10.1007/s40747-018-0072-1\n",
    "4. https://www.kaggle.com/c/bnp-paribas-cardif-claims-management\n",
    "5. University of Washington DATA 558. Z. Harchaoui, C. Jones. 2018 \n",
    "6. Z. Akata, F. Perronnin, Z. Harchaoui and C. Schmid, \"Good Practice in Large-Scale Learning for Image Classification,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 3, pp. 507-520, March 2014.\n",
    "doi: 10.1109/TPAMI.2013.146\n",
    "7. Wang, Yibo & Xu, Wei. (2017). Leveraging deep learning with LDA-based text analytics to detect automobile insurance fraud. Decision Support Systems. 105. 10.1016/j.dss.2017.11.001. \n",
    "8. https://www.google.com/search?q=use+cases+in+the+insurance+industry&ie=utf-8&oe=utf-8&client=firefox-b-1-ab\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
