{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rahul Birmiwal\n",
    "### DATA 558\n",
    "### HW 6\n",
    "\n",
    "** Requires associated modules (See Code Imports below) to run this notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weighted-OVR classifier (as described in Lecture 8) applies a weighting parameter rho to the positive and negative terms of the loss function. \n",
    "\n",
    "$$ \n",
    " \\begin{align*}\n",
    "  J &= \\frac{\\rho}{N_+}\\sum_{i:y_i=1}Loss(y_i,x_i,\\beta)    &+ \\frac{1-\\rho}{N_-}\\sum_{k:y_k=-1}Loss(y_k,x_k,\\beta) + RegTerm\n",
    " \\end{align*}\n",
    "$$\n",
    "* Where N+ is the number of positive ys in the data; likewise for N- \n",
    "* For example, setting rho=0.5 yields the standard 1/N term in front of the summation. \n",
    "\n",
    "The huberized/smooth hinge loss h() writes as in http://qwone.com/~jason/writing/smoothHinge.pdf (2), if we take $ z_i = y_iX_i^T\\beta - 0.5 $\n",
    "\n",
    "It therefore becomes immediate that a weighted cost function for OVR is: \n",
    "$$ \n",
    " \\begin{align*}\n",
    "  J &= \\frac{\\rho}{N_+}\\mathbf{1}^Th(\\mathbf{y_+}*\\mathbf{X_+}^T\\beta)\n",
    "   + \\frac{1-\\rho}{N_-}\\mathbf{1}^Th(\\mathbf{y_-}*\\mathbf{X_-}^T\\beta) + RegTerm\n",
    " \\end{align*}\n",
    "$$\n",
    "Where * denotes the element-wise or Hadamard product\n",
    "\n",
    "And the gradient is: \n",
    "$$ \n",
    " \\begin{align*}\n",
    "  \\nabla{J} &= \\frac{\\rho}{N_+}\\mathbf{X_+}^T\\mathbf{y_+}*(h'\\mathbf(\\mathbf{y_+}*\\mathbf{X_+}^T\\beta))\\\\  +& \n",
    "  \\frac{1-\\rho}{N_-}\\mathbf{X_-}^T\\mathbf{y_-}*(h'\\mathbf(\\mathbf{y_-}*\\mathbf{X_-}^T\\beta)) + 2\\lambda\\beta\n",
    " \\end{align*}\n",
    "$$\n",
    "\n",
    "Where X_+, y_+, etc are the sub-matrices of X,y corresponding to the positive (negative) indices of the vector y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Imports\n",
    "\n",
    "The relevant functions/classes are stored in the following relevant files from below: \n",
    "* loss_functions.py: Various loss and gradients i.e. such as those of huberized hinge/squared hinge/logistic, etc. \n",
    "* RBLinearSVM_variants.py: Class implementation of a Huberized SVM, Squared-Loss, SVM, etc. \n",
    "* multiclass_clfs_variants.py: Class implementation of a weighted-OVR classifier\n",
    "\n",
    "\n",
    "The relevant class protoypes of interest are: \n",
    "\n",
    "```python\n",
    "class Weighted_OVR_Classifier():\n",
    "    def __init__(self, rho, loss_fn , method, reduced_rank = False ):\n",
    "```\n",
    "* Rho: Balancing parameter\n",
    "* Loss_fn: Could be 'huberized_hinge', 'squared_hinge'\n",
    "* Method: Optimizer method, i.e. fastgradalgo or SGD, used for each of its K child OVR classifiers \n",
    "* Reduced_rank: True if to perform SVD dimension reduction on the matrix X to a certain threshold of singular values\n",
    "\n",
    "```python\n",
    "def train_classifiers(self, X_train, y_train, K, do_lambda_tuning, verbose=True,\n",
    "                            X_validate = None, y_validate = None, default_lamb = 1.0):\n",
    "```\n",
    "* Creates K classifiers\n",
    "* Do_lambda_tuning: True if to perform cross-validation to optimize lambda for each of the K classifiers\n",
    "\n",
    "```python\n",
    "def balancer(X_train,y_train, X_validate, y_validate, loss_fn, method, num_classes ):\n",
    "```\n",
    "* Finds optimal value of rho based on scoring on the validation set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'loss_functions' from '/Users/rahulbirmiwal/Documents/UWMSDS/DATA558/Competition2/loss_functions.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "import importlib \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import loss_functions as my_lf\n",
    "import RBLinearSVM_variants as my_svm\n",
    "import multiclass_clfs_variants as my_multi\n",
    "\n",
    "importlib.reload(my_multi)\n",
    "importlib.reload(my_svm)\n",
    "importlib.reload(my_lf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOADING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row.names  y    x.1    x.2    x.3    x.4    x.5    x.6    x.7    x.8  \\\n",
      "0          1  1 -3.639  0.418 -0.670  1.779 -0.168  1.627 -0.388  0.529   \n",
      "1          2  2 -3.327  0.496 -0.694  1.365 -0.265  1.933 -0.363  0.510   \n",
      "2          3  3 -2.120  0.894 -1.576  0.147 -0.707  1.559 -0.579  0.676   \n",
      "3          4  4 -2.287  1.809 -1.498  1.012 -1.053  1.060 -0.567  0.235   \n",
      "4          5  5 -2.598  1.938 -0.846  1.062 -1.633  0.764  0.394 -0.150   \n",
      "\n",
      "     x.9   x.10  \n",
      "0 -0.874 -0.814  \n",
      "1 -0.621 -0.488  \n",
      "2 -0.809 -0.049  \n",
      "3 -0.091 -0.795  \n",
      "4  0.277 -0.396  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "vowels = pd.read_csv('vowels.csv') #training set\n",
    "test_vowels = pd.read_csv('test_vowels.csv') #test set \n",
    "print(vowels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(vowels['y'],dtype=np.int)\n",
    "y_test = np.array(test_vowels['y'],dtype=np.int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(528, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(vowels.iloc[:,2:12], dtype = np.float)\n",
    "X_test = np.array(test_vowels.iloc[:,2:12], dtype = np.float)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train your linear support vector machine with the huberized hinge loss on the the Vowel dataset for the λ = 1. Report your misclassification error for this value of λ.**\n",
    "\n",
    "Let's first find: \n",
    "* 1: Optimal rho, testing in range 0.5,0.10.....1.0 and \n",
    "* 2: Then find misclassification error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### USING RHO = 0.05\n",
      "Running weighted-OVR classifier with rho=0.05\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  27\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  30\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  16\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  30\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  51\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Rho 0.05 had accuracy score of 0.22510822510822512\n",
      "########### USING RHO = 0.1\n",
      "Running weighted-OVR classifier with rho=0.1\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  38\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  42\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  24\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  42\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  69\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Rho 0.1 had accuracy score of 0.22294372294372294\n",
      "########### USING RHO = 0.15000000000000002\n",
      "Running weighted-OVR classifier with rho=0.15000000000000002\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  49\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  54\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  30\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  53\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  82\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Rho 0.15000000000000002 had accuracy score of 0.2532467532467532\n",
      "########### USING RHO = 0.2\n",
      "Running weighted-OVR classifier with rho=0.2\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  57\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  58\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  39\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  62\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  102\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Rho 0.2 had accuracy score of 0.2619047619047619\n",
      "########### USING RHO = 0.25\n",
      "Running weighted-OVR classifier with rho=0.25\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  71\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  73\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  43\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  66\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  117\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Rho 0.25 had accuracy score of 0.2748917748917749\n",
      "########### USING RHO = 0.3\n",
      "Running weighted-OVR classifier with rho=0.3\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  76\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  85\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  46\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  82\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  134\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Rho 0.3 had accuracy score of 0.2857142857142857\n",
      "########### USING RHO = 0.35000000000000003\n",
      "Running weighted-OVR classifier with rho=0.35000000000000003\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  88\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  91\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  58\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  95\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  142\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Rho 0.35000000000000003 had accuracy score of 0.2813852813852814\n",
      "########### USING RHO = 0.4\n",
      "Running weighted-OVR classifier with rho=0.4\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  94\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  112\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  62\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  103\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  172\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Rho 0.4 had accuracy score of 0.2748917748917749\n",
      "########### USING RHO = 0.45\n",
      "Running weighted-OVR classifier with rho=0.45\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  100\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  112\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  68\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  111\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  183\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Rho 0.45 had accuracy score of 0.2813852813852814\n",
      "########### USING RHO = 0.5\n",
      "Running weighted-OVR classifier with rho=0.5\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  114\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  120\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  73\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  125\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  209\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Rho 0.5 had accuracy score of 0.2878787878787879\n",
      "########### USING RHO = 0.55\n",
      "Running weighted-OVR classifier with rho=0.55\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  128\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  128\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  83\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  132\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  236\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Rho 0.55 had accuracy score of 0.2857142857142857\n",
      "########### USING RHO = 0.6000000000000001\n",
      "Running weighted-OVR classifier with rho=0.6000000000000001\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  191\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  138\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  147\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  91\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  140\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  237\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Rho 0.6000000000000001 had accuracy score of 0.29004329004329005\n",
      "########### USING RHO = 0.6500000000000001\n",
      "Running weighted-OVR classifier with rho=0.6500000000000001\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  206\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  156\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  156\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  102\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  139\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  266\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  132\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Rho 0.6500000000000001 had accuracy score of 0.29653679653679654\n",
      "########### USING RHO = 0.7000000000000001\n",
      "Running weighted-OVR classifier with rho=0.7000000000000001\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  213\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  156\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  157\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  102\n",
      "......training for class num  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........number of iterations taken to train =  161\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  289\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Rho 0.7000000000000001 had accuracy score of 0.3051948051948052\n",
      "########### USING RHO = 0.7500000000000001\n",
      "Running weighted-OVR classifier with rho=0.7500000000000001\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  210\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  230\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  169\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  167\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  111\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  170\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  316\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  148\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Rho 0.7500000000000001 had accuracy score of 0.31601731601731603\n",
      "########### USING RHO = 0.8\n",
      "Running weighted-OVR classifier with rho=0.8\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  388\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  236\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  243\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  181\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  177\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  111\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  170\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  315\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  273\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Rho 0.8 had accuracy score of 0.3181818181818182\n",
      "########### USING RHO = 0.8500000000000001\n",
      "Running weighted-OVR classifier with rho=0.8500000000000001\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  235\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  244\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  181\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  177\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  118\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  179\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  334\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  267\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Rho 0.8500000000000001 had accuracy score of 0.3203463203463203\n",
      "########### USING RHO = 0.9000000000000001\n",
      "Running weighted-OVR classifier with rho=0.9000000000000001\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  291\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  256\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  204\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  202\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  126\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  195\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  354\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  182\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  292\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  92\n",
      "...in multiclass predict()\n",
      "Rho 0.9000000000000001 had accuracy score of 0.3203463203463203\n",
      "########### USING RHO = 0.9500000000000001\n",
      "Running weighted-OVR classifier with rho=0.9500000000000001\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  452\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  255\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  230\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  202\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  143\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  223\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  354\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  289\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  91\n",
      "...in multiclass predict()\n",
      "Rho 0.9500000000000001 had accuracy score of 0.3116883116883117\n"
     ]
    }
   ],
   "source": [
    "\n",
    "importlib.reload(my_multi)\n",
    "importlib.reload(my_svm)\n",
    "importlib.reload(my_lf)\n",
    "\n",
    "best_rho, scores , rho_params = my_multi.balancer(X_train,y_train, X_test, y_test, loss_fn='huberized_hinge',\n",
    "                           method='fastgradalgo', num_classes=11 )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of Rho vs Accuracy\n",
    "\n",
    "We plot classification accuracy vs the balancing paramter just to see whether it is significant, and it seems like it is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rho_params, scores) #rho_params, scores were generated from a previous run of this \n",
    "plt.title(\"Balancing Parameter Rho vs. Classification Accuracy\")\n",
    "plt.xlabel(\"Rho\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best rho is 0.8500000000000001\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  235\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  244\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  181\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  177\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  118\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  179\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  334\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  267\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "misclassification error with lambda = 1 is 0.6796536796536796\n"
     ]
    }
   ],
   "source": [
    "print(\"Best rho is {}\".format(best_rho))\n",
    "\n",
    "\"\"\"Instantiate w-OVR classifier\"\"\"\n",
    "wOVR_clf = my_multi.Weighted_OVR_Classifier(best_rho, loss_fn = 'huberized_hinge', method='fastgradalgo')\n",
    "wOVR_clf.train_classifiers(X_train.T,y_train,11,do_lambda_tuning=False, verbose=False)\n",
    "predictions_huber, score = wOVR_clf.predict(X_test.T, y_test)\n",
    "\n",
    "print(\"misclassification error with lambda = 1 is {}\".format(1-score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run cross-validation to find the optimal value of λ. Report your misclassification error for that value of λ.**\n",
    "\n",
    "* We first try tuning lambda on the **local** level, using a global rho value = 0.85 constant across the K classifiers. We do this by setting do_lambda_tuning to True. \n",
    "* Hopefully we get a better error than 68%...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "********* TUNING ITS LAMBDA\n",
      "............Running classifier with lambda=0.0001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.0001 had accuracy score of 0.70995670995671\n",
      "............Running classifier with lambda=0.001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.001 had accuracy score of 0.7359307359307359\n",
      "............Running classifier with lambda=0.01\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.01 had accuracy score of 0.7359307359307359\n",
      "............Running classifier with lambda=0.1\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.1 had accuracy score of 0.6883116883116883\n",
      "............Running classifier with lambda=1\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 1 had accuracy score of 0.6688311688311688\n",
      "............Running classifier with lambda=10\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 10 had accuracy score of 0.6493506493506493\n",
      "............Running classifier with lambda=50\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 50 had accuracy score of 0.6471861471861472\n",
      "&@&@&@&&@&@&@ BEST LAMBDA FOR CLASS 1 is 0.001\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "********* TUNING ITS LAMBDA\n",
      "............Running classifier with lambda=0.0001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.0001 had accuracy score of 0.5735930735930735\n",
      "............Running classifier with lambda=0.001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.001 had accuracy score of 0.5714285714285714\n",
      "............Running classifier with lambda=0.01\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.01 had accuracy score of 0.5952380952380952\n",
      "............Running classifier with lambda=0.1\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.1 had accuracy score of 0.5822510822510822\n",
      "............Running classifier with lambda=1\n",
      "...........number of iterations taken to train =  235\n",
      ".............!!!!!!!!!!!! Lambda 1 had accuracy score of 0.5887445887445888\n",
      "............Running classifier with lambda=10\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 10 had accuracy score of 0.577922077922078\n",
      "............Running classifier with lambda=50\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 50 had accuracy score of 0.5757575757575758\n",
      "&@&@&@&&@&@&@ BEST LAMBDA FOR CLASS 2 is 0.01\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "********* TUNING ITS LAMBDA\n",
      "............Running classifier with lambda=0.0001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.0001 had accuracy score of 0.5757575757575758\n",
      "............Running classifier with lambda=0.001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.001 had accuracy score of 0.5714285714285714\n",
      "............Running classifier with lambda=0.01\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.01 had accuracy score of 0.5411255411255411\n",
      "............Running classifier with lambda=0.1\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.1 had accuracy score of 0.5108225108225108\n",
      "............Running classifier with lambda=1\n",
      "...........number of iterations taken to train =  244\n",
      ".............!!!!!!!!!!!! Lambda 1 had accuracy score of 0.47835497835497837\n",
      "............Running classifier with lambda=10\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 10 had accuracy score of 0.48917748917748916\n",
      "............Running classifier with lambda=50\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 50 had accuracy score of 0.487012987012987\n",
      "&@&@&@&&@&@&@ BEST LAMBDA FOR CLASS 3 is 0.0001\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "********* TUNING ITS LAMBDA\n",
      "............Running classifier with lambda=0.0001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.0001 had accuracy score of 0.5887445887445888\n",
      "............Running classifier with lambda=0.001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.001 had accuracy score of 0.5865800865800865\n",
      "............Running classifier with lambda=0.01\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.01 had accuracy score of 0.5909090909090909\n",
      "............Running classifier with lambda=0.1\n",
      "...........number of iterations taken to train =  44\n",
      ".............!!!!!!!!!!!! Lambda 0.1 had accuracy score of 0.5670995670995671\n",
      "............Running classifier with lambda=1\n",
      "...........number of iterations taken to train =  181\n",
      ".............!!!!!!!!!!!! Lambda 1 had accuracy score of 0.5562770562770563\n",
      "............Running classifier with lambda=10\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 10 had accuracy score of 0.5497835497835498\n",
      "............Running classifier with lambda=50\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 50 had accuracy score of 0.5497835497835498\n",
      "&@&@&@&&@&@&@ BEST LAMBDA FOR CLASS 4 is 0.01\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  5\n",
      "********* TUNING ITS LAMBDA\n",
      "............Running classifier with lambda=0.0001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.0001 had accuracy score of 0.6147186147186147\n",
      "............Running classifier with lambda=0.001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.001 had accuracy score of 0.6277056277056277\n",
      "............Running classifier with lambda=0.01\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.01 had accuracy score of 0.6277056277056277\n",
      "............Running classifier with lambda=0.1\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.1 had accuracy score of 0.5909090909090909\n",
      "............Running classifier with lambda=1\n",
      "...........number of iterations taken to train =  177\n",
      ".............!!!!!!!!!!!! Lambda 1 had accuracy score of 0.5909090909090909\n",
      "............Running classifier with lambda=10\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 10 had accuracy score of 0.5952380952380952\n",
      "............Running classifier with lambda=50\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 50 had accuracy score of 0.5909090909090909\n",
      "&@&@&@&&@&@&@ BEST LAMBDA FOR CLASS 5 is 0.001\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  6\n",
      "********* TUNING ITS LAMBDA\n",
      "............Running classifier with lambda=0.0001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.0001 had accuracy score of 0.6883116883116883\n",
      "............Running classifier with lambda=0.001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.001 had accuracy score of 0.683982683982684\n",
      "............Running classifier with lambda=0.01\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.01 had accuracy score of 0.6536796536796536\n",
      "............Running classifier with lambda=0.1\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.1 had accuracy score of 0.6212121212121212\n",
      "............Running classifier with lambda=1\n",
      "...........number of iterations taken to train =  118\n",
      ".............!!!!!!!!!!!! Lambda 1 had accuracy score of 0.5995670995670995\n",
      "............Running classifier with lambda=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 10 had accuracy score of 0.5995670995670995\n",
      "............Running classifier with lambda=50\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 50 had accuracy score of 0.5995670995670995\n",
      "&@&@&@&&@&@&@ BEST LAMBDA FOR CLASS 6 is 0.0001\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  7\n",
      "********* TUNING ITS LAMBDA\n",
      "............Running classifier with lambda=0.0001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.0001 had accuracy score of 0.5108225108225108\n",
      "............Running classifier with lambda=0.001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.001 had accuracy score of 0.5173160173160173\n",
      "............Running classifier with lambda=0.01\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.01 had accuracy score of 0.525974025974026\n",
      "............Running classifier with lambda=0.1\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.1 had accuracy score of 0.5974025974025974\n",
      "............Running classifier with lambda=1\n",
      "...........number of iterations taken to train =  179\n",
      ".............!!!!!!!!!!!! Lambda 1 had accuracy score of 0.6385281385281385\n",
      "............Running classifier with lambda=10\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 10 had accuracy score of 0.6493506493506493\n",
      "............Running classifier with lambda=50\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 50 had accuracy score of 0.658008658008658\n",
      "&@&@&@&&@&@&@ BEST LAMBDA FOR CLASS 7 is 50\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  8\n",
      "********* TUNING ITS LAMBDA\n",
      "............Running classifier with lambda=0.0001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.0001 had accuracy score of 0.6298701298701299\n",
      "............Running classifier with lambda=0.001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.001 had accuracy score of 0.6190476190476191\n",
      "............Running classifier with lambda=0.01\n",
      "...........number of iterations taken to train =  599\n",
      ".............!!!!!!!!!!!! Lambda 0.01 had accuracy score of 0.6082251082251082\n",
      "............Running classifier with lambda=0.1\n",
      "...........number of iterations taken to train =  117\n",
      ".............!!!!!!!!!!!! Lambda 0.1 had accuracy score of 0.577922077922078\n",
      "............Running classifier with lambda=1\n",
      "...........number of iterations taken to train =  334\n",
      ".............!!!!!!!!!!!! Lambda 1 had accuracy score of 0.6406926406926406\n",
      "............Running classifier with lambda=10\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 10 had accuracy score of 0.670995670995671\n",
      "............Running classifier with lambda=50\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 50 had accuracy score of 0.6731601731601732\n",
      "&@&@&@&&@&@&@ BEST LAMBDA FOR CLASS 8 is 50\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  9\n",
      "********* TUNING ITS LAMBDA\n",
      "............Running classifier with lambda=0.0001\n",
      "...........number of iterations taken to train =  878\n",
      ".............!!!!!!!!!!!! Lambda 0.0001 had accuracy score of 0.6320346320346321\n",
      "............Running classifier with lambda=0.001\n",
      "...........number of iterations taken to train =  785\n",
      ".............!!!!!!!!!!!! Lambda 0.001 had accuracy score of 0.6385281385281385\n",
      "............Running classifier with lambda=0.01\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.01 had accuracy score of 0.6515151515151515\n",
      "............Running classifier with lambda=0.1\n",
      "...........number of iterations taken to train =  139\n",
      ".............!!!!!!!!!!!! Lambda 0.1 had accuracy score of 0.6341991341991342\n",
      "............Running classifier with lambda=1\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 1 had accuracy score of 0.645021645021645\n",
      "............Running classifier with lambda=10\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 10 had accuracy score of 0.6320346320346321\n",
      "............Running classifier with lambda=50\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 50 had accuracy score of 0.6298701298701299\n",
      "&@&@&@&&@&@&@ BEST LAMBDA FOR CLASS 9 is 0.01\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "********* TUNING ITS LAMBDA\n",
      "............Running classifier with lambda=0.0001\n",
      "...........number of iterations taken to train =  580\n",
      ".............!!!!!!!!!!!! Lambda 0.0001 had accuracy score of 0.45021645021645024\n",
      "............Running classifier with lambda=0.001\n",
      "...........number of iterations taken to train =  384\n",
      ".............!!!!!!!!!!!! Lambda 0.001 had accuracy score of 0.45670995670995673\n",
      "............Running classifier with lambda=0.01\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.01 had accuracy score of 0.5021645021645021\n",
      "............Running classifier with lambda=0.1\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.1 had accuracy score of 0.5367965367965368\n",
      "............Running classifier with lambda=1\n",
      "...........number of iterations taken to train =  267\n",
      ".............!!!!!!!!!!!! Lambda 1 had accuracy score of 0.525974025974026\n",
      "............Running classifier with lambda=10\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 10 had accuracy score of 0.5303030303030303\n",
      "............Running classifier with lambda=50\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 50 had accuracy score of 0.525974025974026\n",
      "&@&@&@&&@&@&@ BEST LAMBDA FOR CLASS 10 is 0.1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "********* TUNING ITS LAMBDA\n",
      "............Running classifier with lambda=0.0001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.0001 had accuracy score of 0.5606060606060606\n",
      "............Running classifier with lambda=0.001\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.001 had accuracy score of 0.5692640692640693\n",
      "............Running classifier with lambda=0.01\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.01 had accuracy score of 0.5584415584415584\n",
      "............Running classifier with lambda=0.1\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 0.1 had accuracy score of 0.48484848484848486\n",
      "............Running classifier with lambda=1\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 1 had accuracy score of 0.5216450216450217\n",
      "............Running classifier with lambda=10\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 10 had accuracy score of 0.5151515151515151\n",
      "............Running classifier with lambda=50\n",
      "...........number of iterations taken to train =  1000\n",
      ".............!!!!!!!!!!!! Lambda 50 had accuracy score of 0.512987012987013\n",
      "&@&@&@&&@&@&@ BEST LAMBDA FOR CLASS 11 is 0.001\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "0.28354978354978355\n"
     ]
    }
   ],
   "source": [
    "\"\"\"CROSS-VALIDATION HUBER\"\"\"\n",
    "importlib.reload(my_multi)\n",
    "importlib.reload(my_svm)\n",
    "importlib.reload(my_lf)\n",
    "\n",
    "wOVR_clf = my_multi.Weighted_OVR_Classifier(best_rho, loss_fn = 'huberized_hinge', method='fastgradalgo')\n",
    "wOVR_clf.train_classifiers(X_train = X_train.T, \n",
    "                           y_train = y_train, \n",
    "                           K=11, \n",
    "                           do_lambda_tuning=True, \n",
    "                           verbose=False, \n",
    "                           X_validate = X_test.T, \n",
    "                           y_validate = y_test)\n",
    "predictions_huber, score = wOVR_clf.predict(X_test.T, y_test)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hmmmm...we got an error rate of 72%...WORSE than before. So therefore  keeping rho on the \"global\" level, and minimizing each of K classifiers w.r.t their own lambdas on the \"local\" level, didn't seem to really work. Let's try keeping rho and lambda both on the global level...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### USING LAM = 0.0001, RHO = 0.8500000000000001 \n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  878\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  580\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Lam 0.0001 had accuracy score of 0.31601731601731603\n",
      "########### USING LAM = 0.001, RHO = 0.8500000000000001 \n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  785\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  384\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Lam 0.001 had accuracy score of 0.31601731601731603\n",
      "########### USING LAM = 0.01, RHO = 0.8500000000000001 \n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  599\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Lam 0.01 had accuracy score of 0.3008658008658009\n",
      "########### USING LAM = 0.1, RHO = 0.8500000000000001 \n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  44\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  117\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  139\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Lam 0.1 had accuracy score of 0.3051948051948052\n",
      "########### USING LAM = 1, RHO = 0.8500000000000001 \n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  235\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  244\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  181\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  177\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  118\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  179\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  334\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  267\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Lam 1 had accuracy score of 0.3203463203463203\n",
      "########### USING LAM = 10, RHO = 0.8500000000000001 \n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Lam 10 had accuracy score of 0.2987012987012987\n",
      "########### USING LAM = 50, RHO = 0.8500000000000001 \n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n",
      "Lam 50 had accuracy score of 0.2878787878787879\n"
     ]
    }
   ],
   "source": [
    "\"\"\" LAMDA GLOBAL  BALANCER\"\"\"\n",
    "importlib.reload(my_multi)\n",
    "importlib.reload(my_svm)\n",
    "importlib.reload(my_lf)\n",
    "\n",
    "(best_lamb,scores,lamb_params) = my_multi.balancer_lambda(X_train,y_train, X_test,y_test, best_rho, \n",
    "                                                        'huberized_hinge', 'fastgradalgo', 11 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best global lambda is 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Accuracy')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The best global lambda is {}\".format(best_lamb)) #0.0001\n",
    "plt.plot(lamb_params,scores)\n",
    "plt.xlabel(\"Globalized Lambda with Rho=0.85\")\n",
    "plt.ylabel(\"Accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now train a classifier with lambda = 0.0001 and rho = 0.85 by setting the default_lamb argument**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  878\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  580\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "...in multiclass predict()\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Instantiate w-OVR with lambda = 0.0001 classifier\"\"\"\n",
    "wOVR_clf = my_multi.Weighted_OVR_Classifier(best_rho, loss_fn = 'huberized_hinge', method='fastgradalgo')\n",
    "wOVR_clf.train_classifiers(X_train.T,y_train,11,do_lambda_tuning=False, verbose=False, default_lamb=0.0001)\n",
    "predictions_huber, score = wOVR_clf.predict(X_test.T, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New misclassification error with globally tuned lambda and rho is 0.6839826839826839\n"
     ]
    }
   ],
   "source": [
    "print(\"New misclassification error with globally tuned lambda and rho is {}\".format(1-score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**...so slight improvement after tuning...!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORING PRE-PROCESSING OF THE *X* MATRIX\n",
    "\n",
    "This section is more an optional addendum related to Question 2, pertinent to issues surrounding high-dimensional multiclassification problems. \n",
    "\n",
    "It is well-known that dimensionality reduction (i.e. rank reduction) of the training/testing matrix X can be achived through singular value decomposition. Namely, thresholding a certain number of singular values to zero lowers transforms the original D x N matrix to one of same dimension, but...lower rank. This in turn reduces the number of columns (rows) required in the matrix multiplication of U and V. \n",
    "\n",
    "That is, say we select 6 out of the 10 singular values. Then we need only left multiply by the sigma matrix S by the first 6 eigenvectors of XX^T, and right multiply S by the first 6 eigenvectors of X^TX. This can/should lead to less computationsn required, at the cost of prediction accuracy of course. The question is, how much accuracy is lost in the lower-rank approximation of the matrix X. We examine this below: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd(X, threshold):\n",
    "    (U,S,VH) = np.linalg.svd(X, full_matrices=False)\n",
    "    \"\"\" U is d x n\n",
    "        S is n x 1 vector\n",
    "        VH is n x n\n",
    "    \"\"\"\n",
    "    num_singular_vals = len(S)\n",
    "    thresh = math.floor(threshold*num_singular_vals)\n",
    "    truncated_singular_vals = S[0:thresh]\n",
    "\n",
    "    U_r = U[:,0:thresh]\n",
    "    V_r = VH[0:thresh,]\n",
    "\n",
    "    X_reducedRank = np.matmul(U_r, np.matmul(np.diag(truncated_singular_vals), V_r))\n",
    "    return X_reducedRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 528)\n",
      "(10, 528)\n",
      "(10, 528)\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "A = X_train.T\n",
    "print(A.shape)\n",
    "\n",
    "X_full = svd(A, 1)\n",
    "X_red = svd(A,.6)\n",
    "print(X_full.shape)\n",
    "print(X_red.shape)\n",
    "\n",
    "print(np.allclose(A,X_full))\n",
    "print(np.allclose(A,X_red,atol=0.7))\n",
    "print(np.allclose(A,X_red,atol=1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  250\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  151\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  172\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  120\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  180\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  349\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  161\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "---Reduced Rank 10.471794843673706 seconds ---\n",
      "Instantiaing weighted-one-v-rest classifier using huberized_hinge loss and fastgradalgo optimizer\n",
      "......training for class num  1\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  2\n",
      "...........number of iterations taken to train =  235\n",
      "......training for class num  3\n",
      "...........number of iterations taken to train =  244\n",
      "......training for class num  4\n",
      "...........number of iterations taken to train =  181\n",
      "......training for class num  5\n",
      "...........number of iterations taken to train =  177\n",
      "......training for class num  6\n",
      "...........number of iterations taken to train =  118\n",
      "......training for class num  7\n",
      "...........number of iterations taken to train =  179\n",
      "......training for class num  8\n",
      "...........number of iterations taken to train =  334\n",
      "......training for class num  9\n",
      "...........number of iterations taken to train =  1000\n",
      "......training for class num  10\n",
      "...........number of iterations taken to train =  267\n",
      "......training for class num  11\n",
      "...........number of iterations taken to train =  1000\n",
      "---Full Rank 8.189292192459106 seconds ---\n"
     ]
    }
   ],
   "source": [
    "X_train_red = svd(X_train.T, 0.6)\n",
    "X_test_red = svd(X_test.T, 0.6)\n",
    "import time\n",
    "\n",
    "importlib.reload(my_multi)\n",
    "importlib.reload(my_svm)\n",
    "importlib.reload(my_lf)\n",
    "start_time = time.time()\n",
    "clf_reducedMatrix = my_multi.Weighted_OVR_Classifier(best_rho, loss_fn = 'huberized_hinge', method='fastgradalgo')\n",
    "clf_reducedMatrix.train_classifiers(X_train = X_train_red, \n",
    "                           y_train = y_train, \n",
    "                           K=11, \n",
    "                           do_lambda_tuning=False, \n",
    "                           verbose=False)\n",
    "print(\"---Reduced Rank %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "clf_fullMatrix = my_multi.Weighted_OVR_Classifier(best_rho, loss_fn = 'huberized_hinge', method='fastgradalgo')\n",
    "clf_fullMatrix.train_classifiers(X_train = X_train.T, \n",
    "                           y_train = y_train, \n",
    "                           K=11, \n",
    "                           do_lambda_tuning=False, \n",
    "                        verbose=False)\n",
    "print(\"---Full Rank %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...in multiclass predict()\n",
      "0.3203463203463203\n",
      "...in multiclass predict()\n",
      "0.2683982683982684\n"
     ]
    }
   ],
   "source": [
    "predictions_huber, score_fullRank = clf_fullMatrix.predict(X_test.T, y_test)\n",
    "print(score_fullRank)\n",
    "predictions_huber, score_reducedRank = clf_reducedMatrix.predict(X_test.T, y_test)\n",
    "print(score_reducedRank) #same as score_reducedRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thus thresholding to select the top 60% of singular values led to a ~6 basis point reduction in model accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARISON WITH SKLEARN LINEARSVC\n",
    "\n",
    "Last but not least we should compare our huberized svm with that of SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4090909090909091"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(random_state=0, )\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: \n",
    "\n",
    "* Our maximum accuracy achieved was ~32%. Scikit's counterpart (not huberized hinge, but rather just squared hinge) yields ~40% accuracy. So it seems that we are relatively on par. \n",
    "* The balancing parameter rho significantly compromises/boosts model performance \n",
    "* It seems like SVD can be used to reduce to transform the complexity of the full matrix X to that of lower rank "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
